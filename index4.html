<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Nataniel Ruiz</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        .profile {
            display: flex;
            align-items: center;
            margin-bottom: 40px;
        }
        .profile-info {
            flex: 1;
        }
        .profile-photo {
            width: 200px;
            height: 200px;
            border-radius: 50%;
            margin-left: 20px;
        }
        h1 {
            font-size: 32px;
            margin-bottom: 10px;
        }
        h2 {
            font-size: 24px;
            margin-top: 40px;
            text-transform: uppercase;
        }
        a {
            color: #1a0dab;
            text-decoration: none;
        }
        .publication {
            display: flex;
            margin-bottom: 30px;
        }
        .publication-info {
            flex: 1;
        }
        .publication-image {
            width: 200px;
            height: auto;
            margin-left: 20px;
        }
    </style>
</head>
<body>
    <div class="profile">
        <div class="profile-info">
            <h1>NATANIEL RUIZ</h1>
            <p>Research Scientist, Google</p>
            <p>
                nruiz9 [at] bu.edu / 
                <a href="docs/nataniel_cv_long.pdf">CV</a> / 
                <a href="https://scholar.google.fr/citations?user=CiOmcSIAAAAJ&hl=en">Google Scholar</a> / 
                <a href="https://github.com/natanielruiz">GitHub</a> / 
                <a href="https://www.linkedin.com/in/nataniel-ruiz/">LinkedIn</a>
            </p>
        </div>
        <img src="figs/headshot_circle.png" alt="Nataniel Ruiz" class="profile-photo">
    </div>

    <h2>ABOUT ME</h2>
    <p>I am a Research Scientist at Google. Previously I completed my PhD at <a href="http://www.bu.edu/">Boston University</a>, advised by Professor and Dean of the College of Arts and Sciences <a href="http://www.cs.bu.edu/fac/sclaroff/">Stan Sclaroff</a>. My primary research focus is computer vision and machine learning.</p>
    
    <p>I interned at Amazon working with <a href="https://scholar.google.com/citations?user=Wx62iOsAAAAJ&hl=en">Javier Romero</a>, <a href="https://sites.google.com/site/bolkartt/">Timo Bolkart</a>, <a href="https://www.cs.umd.edu/~lin/">Ming C. Lin</a> and <a href="https://scholar.google.com/citations?user=hY3uN8oAAAAJ&hl=en">Raja Bala</a> during the Summer of 2021. I interned at <a href="https://machinelearning.apple.com/">Apple AI Research</a> during the 2019 and 2020 Summers where I worked with <a href="https://scholar.google.com/citations?user=DNrQd3IAAAAJ&hl=en">Dr. Barry-John Theobald</a> and <a href="https://scholar.google.com/citations?user=p4w7a_kAAAAJ&hl=en">Dr. Nicholas Apostoloff</a>. In 2018 I was a Spring/Summer intern at the <a href="http://www.nec-labs.com/research-departments/media-analytics/media-analytics-home">NEC-Labs Media Analytics Department</a>, where I worked with <a href="http://www.nec-labs.com/~manu/">Prof. Manmohan Chandraker</a> and <a href="http://www.nec-labs.com/samuel-schulter">Dr. Samuel Schulter</a>.</p>

    <h2>PUBLICATIONS</h2>
    
    <div class="publication">
        <div class="publication-info">
            <h3><a href="https://dreambooth.github.io">DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation</a></h3>
            <p><strong>N. Ruiz</strong>, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, K. Aberman</p>
            <p>Conference on Computer Vision and Pattern Recognition (CVPR), 2023</p>
            <p><strong>(Best Student Paper Honorable Mention Award - 0.25% award rate)</strong></p>
            <p>
                <a href="https://dreambooth.github.io">Website</a> /
                <a href="https://en.wikipedia.org/wiki/DreamBooth">Wikipedia</a> /
                <a href="https://www.youtube.com/watch?v=W4Mcuh38wyM">Corridor Crew demo video</a> /
                <a href="https://twitter.com/natanielruizg/status/1563166568195821569?s=20&t=vAnQmQb7iYVoKTzgblFxYg">Announcement tweet</a>
            </p>
        </div>
        <img src="figs/dreambooth.png" alt="DreamBooth" class="publication-image">
    </div>

    <div class="publication">
        <div class="publication-info">
            <h3><a href="https://counterfactualsimulation.github.io">Finding Differences Between Transformers and ConvNets Using Counterfactual Simulation Testing</a></h3>
            <p><strong>N. Ruiz</strong>, S. Bargal, Cihang Xie, Kate Saenko, S. Sclaroff</p>
            <p>Conference on Neural Information Processing Systems (NeurIPS), 2022</p>
            <p><a href="https://counterfactualsimulation.github.io">Website</a></p>
        </div>
        <img src="figs/counterfactual_teaser.png" alt="Counterfactual Simulation" class="publication-image">
    </div>

    <div class="publication">
        <div class="publication-info">
            <h3><a href="https://adversarialbodysim.github.io">Human Body Measurement Estimation with Adversarial Augmentation</a></h3>
            <p><strong>N. Ruiz</strong>, M. Bellver, T. Bolkart, A. Arora, M.C. Lin, J. Romero, R. Bala</p>
            <p>International Conference on 3D Vision (3DV), 2022</p>
            <p>
                <a href="https://adversarialbodysim.github.io">Website</a> /
                <a href="https://adversarialbodysim.github.io/files/148.pdf">Poster</a> /
                <a href="https://drive.google.com/file/d/1uyrSuhsHYiiTqr68yQb3Rf3OawFl_a3F/view?usp=sharing">Video</a> /
                <a href="https://github.com/awslabs/open-data-registry/blob/main/datasets/bodym.yaml">Dataset</a>
            </p>
        </div>
        <img src="figs/model_amazon.png" alt="Human Body Measurement" class="publication-image">
    </div>

    <div class="publication">
        <div class="publication-info">
            <h3><a href="https://arxiv.org/abs/2106.04569">Simulated Adversarial Testing of Face Recognition Models</a></h3>
            <p><strong>N. Ruiz</strong>, A. Kortylewski, W. Qiu, Cihang Xie, Sarah Adel Bargal, Alan Yuille, S. Sclaroff</p>
            <p>Conference on Computer Vision and Pattern Recognition (CVPR), 2022</p>
            <p><a href="https://arxiv.org/pdf/2106.04569.pdf">PDF</a></p>
        </div>
        <img src="figs/adv_test_sim.png" alt="Adversarial Testing" class="publication-image">
    </div>

    <div class="publication">
        <div class="publication-info">
            <h3><a href="https://arxiv.org/abs/2107.09126">Examining the Human Perceptibility of Black-Box Adversarial Attacks on Face Recognition</a></h3>
            <p>Benjamin Spetter-Goldstein, <strong>N. Ruiz</strong>, Sarah Adel Bargal</p>
            <p>ICML Adversarial Machine Learning Workshop, 2021</p>
            <p><a href="https://arxiv.org/pdf/2107.09126.pdf">PDF</a></p>
        </div>
        <img src="figs/bbox_face_attacks.png" alt="Black-Box Adversarial Attacks" class="publication-image">
    </div>

    <div class="publication">
        <div class="publication-info">
            <h3><a href="https://arxiv.org/abs/2012.05225">MorphGAN: One-Shot Face Synthesis GAN for Detecting Recognition Bias</a></h3>
            <p><strong>N. Ruiz</strong>, B. J. Theobald, A. Ranjan, A. H. Abdelaziz, N. Apostoloff</p>
            <p>British Machine Vision Conference (BMVC), 2021</p>
            <p><a href="https://arxiv.org/pdf/2012.05225.pdf">PDF</a></p>
        </div>
        <img src="figs/morphgan.gif" alt="MorphGAN" class="publication-image">
    </div>

    <div class="publication">
        <div class="publication-info">
            <h3><a href="https://arxiv.org/abs/2006.06493">Protecting Against Image Translation Deepfakes by Leaking Universal Perturbations from Black-Box Neural Networks</a></h3>
            <p><strong>N. Ruiz</strong>, S. Bargal, S. Sclaroff</p>
            <p>arXiv, 2021</p>
            <p><a href="https://arxiv.org/pdf/2006.06493.pdf">PDF</a></p>
        </div>
        <img src="figs/blackbox_deepfake.png" alt="Deepfake Protection" class="publication-image">
    </div>

    <div class="publication">
        <div class="publication-info">
            <h3><a href="https://arxiv.org/abs/2002.05242">Leveraging Affect Transfer Learning for Behavior Prediction in an Intelligent Tutoring System</a></h3>
            <p><strong>N. Ruiz</strong>, H. Yu, D. Allessio, M. Jalal, Thomas Murray, J. Magee, J. Whitehill, V. Ablavsky, I. Arroyo, B. Woolf, S. Sclaroff, M. Betke</p>
            <p>IEEE International Conference on Automatic Face and Gesture Recognition (FG), 2021</p>
            <p>IEEE Transactions on Biometrics, Identity and Behavior (T-BIOM), 2022</p>
            <p><strong>(Selected for Oral Presentation and Best Poster Award - 4% award rate)</strong></p>
            <p><a href="https://arxiv.org/pdf/2002.05242.pdf">PDF</a> / <a href="https://ieeexplore.ieee.org/abstract/document/9906410">Journal paper</a></p>
        </div>
        <img src="figs/img_leveraging.png" alt="Affect Transfer Learning" class="publication-image">
    </div>

    <div class="publication">
        <div class="publication-info">
            <h3><a href="https://arxiv.org/abs/2003.01279">Disrupting DeepFakes: Adversarial Attacks Against Conditional Image Translation Networks and Facial Manipulation Systems</a></h3>
            <p><strong>N. Ruiz</strong>, S. Bargal, S. Sclaroff</p>
            <p>CVPR Workshop on Adversarial Machine Learning in Computer Vision and ECCV Workshop on Advances in Image Manipulation, 2020</p>
            <p>
                <a href="https://arxiv.org/pdf/2003.01279.pdf">PDF</a> /
                <a href="https://twimlai.com/twiml-talk-375-disrupting-deepfakes-adversarial-attacks-against-conditional-image-translation-networks-with-nataniel-ruiz/">Podcast</a> /
                <a href="https://github.com/natanielruiz/disrupting-deepfakes">Code</a> /
                <a href="https://www.youtube.com/watch?v=7_7r4Ng4-bE&feature=youtu.be">Video demo</a>
            </p>
        </div>
        <img src="figs/img_disrupt.png" alt="Disrupting DeepFakes" class="publication-image">
    </div>

    <div class="publication">
        <div class="publication-info">
            <h3><a href="https://arxiv.org/abs/2003.02501">Detecting Attended Visual Targets in Video</a></h3>
            <p>E. Chong, Y. Wang, <strong>N. Ruiz</strong>, J.M. Rehg</p>
            <p>Conference on Computer Vision and Pattern Recognition (CVPR), 2020</p>
            <p><a href="https://arxiv.org/pdf/2003.02501.pdf">PDF</a> / <a href="https://github.com/ejcgt/attention-target-detection">Code</a></p>
        </div>
        <img src="figs/img_visual_target_video.png" alt="Detecting Visual Targets" class="publication-image">
    </div>

    <div class="publication">
      <div class="publication-info">
          <h3><a href="https://arxiv.org/abs/1810.02513">Learning To Simulate</a></h3>
          <p><strong>N. Ruiz</strong>, S. Schulter, M. Chandraker</p>
          <p>International Conference on Learning Representations (ICLR), 2019</p>
          <p><a href="https://arxiv.org/pdf/1810.02513.pdf">PDF</a> / <a href="docs/lts_poster.pdf">Poster</a></p>
      </div>
      <img src="figs/lts_sim.png" alt="Learning To Simulate" class="publication-image">
  </div>

  <div class="publication">
      <div class="publication-info">
          <h3><a href="http://openaccess.thecvf.com/content_ECCV_2018/html/Eunji_Chong_Connecting_Gaze_Scene_ECCV_2018_paper.html">Connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency</a></h3>
          <p>E. Chong, <strong>N. Ruiz</strong>, Y. Wang, Y. Zhang, A. Rozga, J.M. Rehg</p>
          <p>European Conference on Computer Vision (ECCV), 2018</p>
          <p>
              <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Eunji_Chong_Connecting_Gaze_Scene_ECCV_2018_paper.pdf">PDF</a> /
              <a href="docs/argos_poster.pdf">Poster</a> /
              <a href="docs/Chong_2018_ECCV.bib">Bibtex</a>
          </p>
      </div>
      <img src="figs/gaze.png" alt="Gaze and Scene Saliency" class="publication-image">
  </div>

  <div class="publication">
      <div class="publication-info">
          <h3><a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/w41/html/Ruiz_Fine-Grained_Head_Pose_CVPR_2018_paper.html">Fine-Grained Head Pose Estimation Without Keypoints</a></h3>
          <p><strong>N. Ruiz</strong>, E. Chong, J.M. Rehg</p>
          <p>Conference on Computer Vision and Pattern Recognition Workshop (CVPRW), 2018 <strong>(Oral Presentation)</strong></p>
          <p>
              <a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w41/Ruiz_Fine-Grained_Head_Pose_CVPR_2018_paper.pdf">PDF</a> /
              <a href="https://github.com/natanielruiz/deep-head-pose">Code</a> /
              <a href="https://youtu.be/Bz6eF4Nl1O8">Video demo</a> /
              <a href="docs/hopenet_poster.pdf">Poster</a> /
              <a href="docs/ruiz2017fine.bib">Bibtex</a>
          </p>
      </div>
      <img src="figs/deep-head-pose.png" alt="Head Pose Estimation" class="publication-image">
  </div>

  <div class="publication">
      <div class="publication-info">
          <h3><a href="https://arxiv.org/abs/1809.08381">Learning to Localize and Align Fine-Grained Actions to Sparse Instructions</a></h3>
          <p>M. Hahn, <strong>N. Ruiz</strong>, J.B. Alayrac, I. Laptev, J.M. Rehg</p>
          <p>arXiv, 2018</p>
          <p><a href="https://arxiv.org/pdf/1809.08381.pdf">PDF</a></p>
      </div>
      <img src="figs/localize-and-align.png" alt="Localize and Align Actions" class="publication-image">
  </div>

  <div class="publication">
      <div class="publication-info">
          <h3><a href="https://dl.acm.org/citation.cfm?id=3131902">Detecting Gaze Towards Eyes in Natural Social Interactions and Its Use in Child Assessment</a></h3>
          <p>E. Chong, K. Chanda, Z. Ye, A. Southerland, <strong>N. Ruiz</strong>, R.M. Jones, A. Rozga, J.M. Rehg</p>
          <p>UbiComp and Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT), 2017</p>
          <p><strong>(Selected for Oral Presentation and Distinguished Paper Award - 3% award rate)</strong></p>
          <p><a href="https://dl.acm.org/doi/pdf/10.1145/3131902">PDF</a> / <a href="docs/chong2017imwut.bib">Bibtex</a></p>
      </div>
      <img src="figs/eye-contact.png" alt="Detecting Gaze Towards Eyes" class="publication-image">
  </div>

  <div class="publication">
      <div class="publication-info">
          <h3><a href="https://arxiv.org/abs/1708.04370">Dockerface: an Easy to Install and Use Faster R-CNN Face Detector in a Docker Container</a></h3>
          <p><strong>N. Ruiz</strong>, J.M. Rehg</p>
          <p>arXiv, 2017</p>
          <p><a href="https://arxiv.org/pdf/1708.04370.pdf">PDF</a> / <a href="https://github.com/natanielruiz/dockerface">Code</a> / <a href="docs/ruiz2017dockerface.bib">Bibtex</a></p>
      </div>
      <img src="figs/dockerface.jpg" alt="Dockerface" class="publication-image">
  </div>

  <h2>PROJECTS</h2>

  <div class="publication">
      <div class="publication-info">
          <h3><a href="https://github.com/natanielruiz/android-yolo">android-yolo</a></h3>
          <p><strong>N. Ruiz</strong></p>
          <p>Real-time object detection on Android using the YOLO network with TensorFlow.</p>
          <p>
              <a href="https://github.com/natanielruiz/android-yolo">GitHub</a> /
              <a href="https://youtu.be/EhMrf4G5Wf0">Video demo</a> /
              <a href="https://drive.google.com/open?id=0B2fFW2t9-qW3LWFDNXVHUE9rV3M">App APK</a>
          </p>
      </div>
      <img src="figs/yolo.png" alt="android-yolo" class="publication-image">
  </div>

</body>
</html>