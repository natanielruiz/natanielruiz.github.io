<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en">
<head>
    <meta charset="UTF-8">
    <!-- <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"> -->
    <meta name=viewport content=“width=1000>
    <title>Nataniel Ruiz</title>
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600;700&display=swap" rel="stylesheet">
    <style>
        body, td, th, tr, p, a {
            font-family: 'Poppins', sans-serif;
            font-size: 14px;
        }
        strong {
            font-family: 'Poppins', sans-serif;
            font-size: 14px;
        }
        heading {
            font-family: 'Poppins', sans-serif;
            font-size: 22px;
            font-weight: 600;
        }
        papertitle {
            font-family: 'Poppins', sans-serif;
            font-size: 14px;
            font-weight: 600;
        }
        name {
            font-family: 'Poppins', sans-serif;
            font-size: 32px;
        }
        .one {
            width: 160px;
            height: 160px;
            position: relative;
        }
        .two {
            width: 160px;
            height: 160px;
            position: absolute;
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }
        .fade {
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }
        span.highlight {
            background-color: #ffffd0;
        }
    </style>
</head>
<body>
    <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
        <tr>
            <td>
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td width="75%" valign="middle">
                            <p align="center">
                                <name>Nataniel Ruiz</name>
                            </p>
                            <p style="text-align:justify">
                                I am a Research Scientist at <b>Google</b>. I completed my PhD at <a href="http://www.bu.edu/">Boston University</a>, advised by Professor and Dean of the College of Arts and Sciences <a href="http://www.cs.bu.edu/fac/sclaroff/">Stan Sclaroff</a>. I did my Masters at <a href="https://www.gatech.edu/">Georgia Tech</a> with <a href="https://rehg.org/">James Rehg</a> and my Bachelors at <a href="https://www.polytechnique.edu/en">Ecole Polytechnique</a> in Paris. Prior to joining Google I interned at Apple, Amazon and NEC Labs. My primary research focus is computer vision and machine learning.
                            </p>
                            <p align="center">
                                nruiz9 [at] bu.edu |
                                <a href="docs/nataniel_cv_long.pdf">CV</a> |
                                <a href="https://scholar.google.fr/citations?user=CiOmcSIAAAAJ&hl=en">Google Scholar</a> |
                                <a href="https://github.com/natanielruiz">GitHub</a> |
                                <a href="https://www.linkedin.com/in/nataniel-ruiz/"> LinkedIn </a>
                            </p>
                        </td>
                        <td width="25%">
                            <img src="figs/headshot_circle.png" alt="Nataniel Ruiz">
                        </td>
                    </tr>
                </table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td width="100%" valign="middle">
                            <heading>Research</heading>
                            <p>
                                Currently, my main interests include generative models, diffusion models, personalization of generative models, simulation and beneficial adversarial attacks. 
                            </p>
                        </td>
                    </tr>
                </table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td>
                            <heading>Publications</heading>
                        </td>
                    </tr>
                </table>

                <!-- Magic Insert: Style-Aware Drag-and-Drop -->
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td width="25%">
                            <img src='figs/magicinsert.png' width="200" height="200">
                        </td>
                        <td width="75%" valign="top">
                            <p>
                                <a href="https://arxiv.org/abs/2407.02489">
                                    <papertitle>Magic Insert: Style-Aware Drag-and-Drop</papertitle>
                                </a>
                                <br>
                                <strong>Nataniel Ruiz</strong>, Yuanzhen Li, Neal Wadhwa, Yael Pritch, Michael Rubinstein, David E. Jacobs, Shlomi Fruchter
                                <br>
                                <em>arXiv preprint arXiv:2407.02489</em>, 2024
                                <br>
                                <a href="https://magicinsert.github.io/">website</a> |
                                <a href="https://magicinsert.github.io/demo.html">demo</a> |
                                <a href="https://magicinsert.github.io/subjectplop.zip">dataset</a> |
                                <a href="https://x.com/natanielruizg/status/1808524800630960497">tweet</a>
                            </p>
                        </td>
                    </tr>
                </table>


                <!-- RB-Modulation: Training-Free Personalization of Diffusion Models using Stochastic Optimal Control -->
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td width="25%">
                            <img src='figs/rbmodulation.png' width="200" height="200">
                        </td>
                        <td width="75%" valign="top">
                            <p>
                                <a href="https://arxiv.org/abs/2405.17401">
                                    <papertitle>RB-Modulation: Training-Free Personalization of Diffusion Models using Stochastic Optimal Control</papertitle>
                                </a>
                                <br>
                                Litu Rout, Yujia Chen, <strong>Nataniel Ruiz</strong>, Abhishek Kumar, Constantine Caramanis, Sanjay Shakkottai, Wen-Sheng Chu
                                <br>
                                <em>arXiv preprint arXiv:2405.17401</em>, 2024
                                <br>
                                <a href="https://rb-modulation.github.io/">website</a> |
                                <a href="https://huggingface.co/spaces/fffiloni/RB-Modulation">demo</a> |
                                <a href="https://github.com/google/RB-Modulation">code</a> |
                                <a href="https://x.com/natanielruizg/status/1796217919807934475">tweet</a>
                            </p>
                        </td>
                    </tr>
                </table>

                <!-- ZipLoRA: Any Subject in Any Style by Effectively Merging LoRAs -->
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td width="25%">
                            <img src='figs/ziplora.png' width="200" height="200">
                        </td>
                        <td width="75%" valign="top">
                            <p>
                                <a href="https://arxiv.org/abs/2311.13600">
                                    <papertitle>ZipLoRA: Any Subject in Any Style by Effectively Merging LoRAs</papertitle>
                                </a>
                                <br>
                                Viraj Shah, <strong>Nataniel Ruiz</strong>, Forrester Cole, Erika Lu, Svetlana Lazebnik, Yuanzhen Li, Varun Jampani
                                <br>
                                <em>European Conference on Computer Vision (ECCV)</em>, 2024
                                <br>
                                <a href="https://ziplora.github.io/">website</a> |
                                <a href="https://x.com/natanielruizg/status/1727718489425616912">tweet</a> |
                                <a href="https://github.com/mkshing/ziplora-pytorch">unofficial code</a>
                            </p>
                        </td>
                    </tr>
                </table>

                <!-- RealFill: Reference-driven Generation for Authentic Image Completion -->
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td width="25%">
                            <img src='figs/realfill.png' width="200" height="200">
                        </td>
                        <td width="75%" valign="top">
                            <p>
                                <a href="https://arxiv.org/abs/2309.16668">
                                    <papertitle>RealFill: Reference-driven Generation for Authentic Image Completion</papertitle>
                                </a>
                                <br>
                                Luming Tang, <strong>Nataniel Ruiz</strong>, Qinghao Chu, Yuanzhen Li, Aleksander Holynski, David E. Jacobs, Bharath Hariharan, Yael Pritch, Neal Wadhwa, Kfir Aberman, Michael Rubinstein
                                <br>
                                <em>SIGGRAPH (Journal Track)</em>, 2024
                                <br>
                                <a href="https://realfill.github.io/">website</a> |
                                <a href="https://www.youtube.com/watch?v=bD_HyxHMHPo">two minute papers</a> |
                                <a href="https://drive.google.com/file/d/18hlxyw3g3C9QwG9PtFCPedMgcVxFAN0e/view">data</a> |
                                <a href="https://x.com/natanielruizg/status/1707767757503103031">tweet</a> |
                                <a href="https://github.com/thuanz123/realfill">code</a>
                            </p>
                        </td>
                    </tr>
                </table>

                <!-- HyperDreamBooth: Hypernetworks for Fast Personalization of Text-to-Image Models -->
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td width="25%">
                            <img src='figs/hyperdreambooth.png' width="200" height="200">
                        </td>
                        <td width="75%" valign="top">
                            <p>
                                <a href="https://arxiv.org/abs/2307.06949">
                                    <papertitle>HyperDreamBooth: Hypernetworks for Fast Personalization of Text-to-Image Models</papertitle>
                                </a>
                                <br>
                                <strong>Nataniel Ruiz</strong>, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, Kfir Aberman
                                <br>
                                <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2024
                                <br>
                                <a href="https://hyperdreambooth.github.io/">website</a> |
                                <a href="https://x.com/natanielruizg/status/1679893292618752000">tweet</a>
                            </p>
                        </td>
                    </tr>
                </table>

                <!-- Platypus -->
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td width="25%">
                            <img src='figs/platypus.jpeg' width="200" height="200">
                        </td>
                        <td width="75%" valign="top">
                            <p>
                                <a href="https://arxiv.org/abs/2308.07317">
                                    <papertitle>Platypus: Quick, Cheap, and Powerful Refinement of LLMs</papertitle>
                                </a>
                                <br>
                                Ariel N. Lee, Cole J. Hunter, <strong>Nataniel Ruiz</strong>
                                <br>
                                <em>Neural Information Processing Systems Workshop (NeurIPS Workshop​)​</em>, 2023
                                <br>
                                <a href="https://platypus-llm.github.io/">website</a> |
                                <a href="https://x.com/natanielruizg/status/1690048207030493189">tweet</a> |
                                <a href="https://huggingface.co/garage-bAInd">models</a> |
                                <a href="https://huggingface.co/datasets/garage-bAInd/Open-Platypus">dataset</a> |
                                <a href="https://github.com/arielnlee/Platypus">code</a>
                            </p>
                        </td>
                    </tr>
                </table>

                <!-- SuTI: Subject-driven Text-to-Image Generation via Apprenticeship Learning -->
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td width="25%">
                            <img src='figs/suti.png' width="200" height="200">
                        </td>
                        <td width="75%" valign="top">
                            <p>
                                <a href="https://arxiv.org/abs/2304.00186">
                                    <papertitle>SuTI: Subject-driven Text-to-Image Generation via Apprenticeship Learning</papertitle>
                                </a>
                                <br>
                                Wenhu Chen, Hexiang Hu, Yuanzhen Li, <strong>Nataniel Ruiz</strong>, Xuhui Jia, Ming-Wei Chang, William W. Cohen
                                <br>
                                <em>Neural Information Processing Systems (NeurIPS)</em>, 2023
                                <br>
                                <a href="https://open-vision-language.github.io/suti/">website</a> |
                                <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/image/fine-tune-model">google cloud launch</a>
                            </p>
                        </td>
                    </tr>
                </table>

                <!-- StyleDrop -->
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td width="25%">
                            <img src='figs/styledrop.png' width="200" height="200">
                        </td>
                        <td width="75%" valign="top">
                            <p>
                                <a href="https://arxiv.org/abs/2306.00983">
                                    <papertitle>StyleDrop: Text-to-Image Generation in Any Style</papertitle>
                                </a>
                                <br>
                                Kihyuk Sohn, <strong>Nataniel Ruiz</strong>, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang, Glenn Entis, Yuanzhen Li, Yuan Hao, Irfan Essa, Michael Rubinstein, Dilip Krishnan
                                <br>
                                <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 2023
                                <br>
                                <a href="https://styledrop.github.io">website</a> |
                                <a href="https://x.com/natanielruizg/status/1664653573597065225">tweet</a> |
                                <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/image/fine-tune-style">google cloud launch</a> |
                                <a href="https://github.com/zideliu/StyleDrop-PyTorch">unofficial code</a>
                            </p>
                        </td>
                    </tr>
                </table>

                <!-- DreamBooth3D -->
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td width="25%">
                            <img src='figs/dreambooth3d.png' width="200" height="200">
                        </td>
                        <td width="75%" valign="top">
                            <p>
                                <a href="https://arxiv.org/abs/2303.13508">
                                    <papertitle>DreamBooth3D: Subject-driven Text-to-3D Generation</papertitle>
                                </a>
                                <br>
                                Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, <strong>Nataniel Ruiz</strong>, Ben Mildenhall, Shiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, Yuanzhen Li, Varun Jampani
                                <br>
                                <em>International Conference on Computer Vision (ICCV)</em>, 2023
                                <br>
                                <a href="https://dreambooth3d.github.io/">website</a> |
                                <a href="https://www.youtube.com/watch?v=kKVDrbfvOoA">demo video</a> |
                                <a href="https://x.com/_akhaliq/status/1639066078323298304">tweet</a>
                            </p>
                        </td>
                    </tr>
                </table>

                <!-- DreamBooth -->
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td width="25%">
                            <img src='figs/dreambooth.png' width="200" height="200">
                        </td>
                        <td width="75%" valign="top">
                            <p>
                                <a href="https://arxiv.org/abs/2208.12242">
                                    <papertitle>DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation</papertitle>
                                </a>
                                <br>
                                <strong>Nataniel Ruiz</strong>, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman
                                <br>
                                <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023
                                <font color="red"><strong>(Best Student Paper Honorable Mention Award - 0.25% award rate)</strong></font>
                                <br>
                                <a href="https://dreambooth.github.io">website</a> |
                                <a href="https://en.wikipedia.org/wiki/DreamBooth">wikipedia</a> |
                                <a href="https://www.youtube.com/watch?v=W4Mcuh38wyM">corridor crew video</a> |
                                <a href="https://twitter.com/natanielruizg/status/1563166568195821569?s=20&t=vAnQmQb7iYVoKTzgblFxYg">tweet</a> |
                                <a href="https://github.com/huggingface/diffusers/tree/main/examples/dreambooth">diffusers code</a> |
                                <a href="https://github.com/XavierXiao/Dreambooth-Stable-Diffusion">alt unofficial code</a> | 
                                <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/image/fine-tune-model">google cloud launch</a>
                            </p>
                        </td>
                    </tr>
                </table>

                <!-- Simulating to Learn: Using Adaptive Simulation to Train, Test and Understand Neural Networks (Thesis) -->
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td width="25%">
                            <img src='figs/simulatingtolearn.png' width="200" height="200">
                        </td>
                        <td width="75%" valign="top">
                            <p>
                                <a href="https://www.proquest.com/openview/95d9950a23ab02ce22436eb1862c4fbf/1?pq-origsite=gscholar&cbl=18750&diss=y">
                                    <papertitle>Simulating to Learn: Using Adaptive Simulation to Train, Test and Understand Neural Networks</papertitle>
                                </a>
                                <br>
                                <strong>Nataniel Ruiz</strong>
                                <br>
                                <em>Boston University, 2023 (PhD Thesis)</em>
                                <br>
                                This thesis presents new insights into the use of adaptive simulation to train and test machine learning models, addressing the key obstacle of collecting annotated and high-quality real-world data. It presents five novel methods for adapting simulated data distributions to improve the training and testing of neural networks.
                            </p>
                        </td>
                    </tr>
                </table>

                <!-- Practical Disruption of Image Translation Deepfake Networks -->
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td width="25%">
                            <img src='figs/practicaldisruptions.png' width="200" height="200">
                        </td>
                        <td width="75%" valign="top">
                            <p>
                                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/26693">
                                    <papertitle>Practical Disruption of Image Translation Deepfake Networks</papertitle>
                                </a>
                                <br>
                                <strong>Nataniel Ruiz</strong>, Sarah A. Bargal, Cihang Xie, Stan Sclaroff
                                <br>
                                <em>AAAI Conference on Artificial Intelligence</em>, 2023
                                <br>
                            </p>
                        </td>
                    </tr>
                </table>


                <!-- Hardwiring ViT Patch Selectivity into CNNs using Patch Mixing -->
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td width="25%">
                            <img src='figs/hardwiring.png' width="200" height="200">
                        </td>
                        <td width="75%" valign="top">
                            <p>
                                <a href="https://arxiv.org/abs/2306.17848">
                                    <papertitle>Hardwiring ViT Patch Selectivity into CNNs using Patch Mixing</papertitle>
                                </a>
                                <br>
                                Ariel N. Lee, Sarah A. Bargal, Janavi Kasera, Stan Sclaroff, Kate Saenko, <strong>Nataniel Ruiz</strong>
                                <br>
                                <em>arXiv preprint arXiv:2306.17848</em>, 2023
                                <br>
                                <a href="https://arielnlee.github.io/PatchMixing/">website</a> |
                                <a href="https://huggingface.co/datasets/ariellee/Superimposed-Masked-Dataset">Superimposed Masked Dataset</a> |
                                <a href="https://huggingface.co/datasets/ariellee/Realistic-Occlusion-Dataset">Realistic Occlusion Dataset</a> |
                                <a href="https://x.com/natanielruizg/status/1675899509396631555">tweet</a>
                            </p>
                        </td>
                    </tr>
                </table>

                <!-- Counterfactual Simulation Testing -->
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td width="25%">
                            <img src='figs/counterfactual_teaser.png' width="200" height="200">
                        </td>
                        <td width="75%" valign="top">
                            <p>
                                <a href="https://arxiv.org/abs/2211.16499">
                                    <papertitle>Finding Differences Between Transformers and ConvNets Using Counterfactual Simulation Testing</papertitle>
                                </a>
                                <br>
                                <strong>Nataniel Ruiz</strong>, Sarah A. Bargal, Cihang Xie, Kate Saenko, Stan Sclaroff
                                <br>
                                <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 2022
                                <br>
                                <a href="https://counterfactualsimulation.github.io">website</a> |
                                <a href="https://drive.google.com/drive/folders/1uS0XlVtoVy-B0yuVI7bFvBQb3RiCeeE3?usp=share_link">Dataset</a>
                            </p>
                        </td>
                    </tr>
                </table>

                <!-- Human Body Measurement Estimation -->
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td width="25%">
                            <img src='figs/model_amazon.png' width="200" height="200">
                        </td>
                        <td width="75%" valign="top">
                            <p>
                                <a href="https://arxiv.org/abs/2210.05667">
                                    <papertitle>Human Body Measurement Estimation with Adversarial Augmentation</papertitle>
                                </a>
                                <br>
                                <strong>Nataniel Ruiz</strong>, Miriam Bellver, Timo Bolkart, Ambuj Arora, Ming C. Lin, Javier Romero, Raja Bala
                                <br>
                                <em>International Conference on 3D Vision (3DV)</em>, 2022
                                <br>
                                <a href="https://adversarialbodysim.github.io">website</a> |
                                <a href="https://adversarialbodysim.github.io/files/148.pdf">poster</a> |
                                <a href="https://drive.google.com/file/d/1uyrSuhsHYiiTqr68yQb3Rf3OawFl_a3F/view?usp=sharing">video</a> |
                                <a href="https://github.com/awslabs/open-data-registry/blob/main/datasets/bodym.yaml">dataset</a>
                            </p>
                        </td>
                    </tr>
                </table>

                <!-- Simulated Adversarial Testing -->
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td width="25%">
                            <img src='figs/adv_test_sim.png' width="200" height="200">
                        </td>
                        <td width="75%" valign="top">
                            <p>
                                <a href="https://arxiv.org/abs/2106.04569">
                                    <papertitle>Simulated Adversarial Testing of Face Recognition Models</papertitle>
                                </a>
                                <br>
                                <strong>Nataniel Ruiz</strong>, Adam Kortylewski, Weichao Qiu, Cihang Xie, Sarah A. Bargal, Alan Yuille, Stan Sclaroff
                                <br>
                                <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022
                            </p>
                        </td>
                    </tr>
                </table>

                <!-- MorphGAN -->
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td width="25%">
                            <img src='figs/morphgan.gif' width="200" height="200">
                        </td>
                        <td width="75%" valign="top">
                            <p>
                                <a href="https://arxiv.org/abs/2012.05225">
                                    <papertitle>MorphGAN: One-Shot Face Synthesis GAN for Detecting Recognition Bias</papertitle>
                                </a>
                                <br>
                                <strong>Nataniel Ruiz</strong>, Barry-John Theobald, Anurag Ranjan, Ahmed H. Abdelaziz, Nicholas Apostoloff
                                <br>
                                <em>British Machine Vision Conference (BMVC)</em>, 2021
                            </p>
                        </td>
                    </tr>
                </table>

                <!-- Protecting Against Image Translation Deepfakes -->
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td width="25%">
                            <img src='figs/blackbox_deepfake.png' width="200" height="200">
                        </td>
                        <td width="75%" valign="top">
                            <p>
                                <a href="https://arxiv.org/abs/2006.06493">
                                    <papertitle>Protecting Against Image Translation Deepfakes by Leaking Universal Perturbations from Black-Box Neural Networks</papertitle>
                                </a>
                                <br>
                                <strong>Nataniel Ruiz</strong>, Sarah A. Bargal, Stan Sclaroff
                                <br>
                                <em>arxiv</em>, 2021
                            </p>
                        </td>
                    </tr>
                </table>

                <!-- Leveraging Affect Transfer Learning -->
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td width="25%">
                            <img src='figs/img_leveraging.png' width="200" height="200">
                        </td>
                        <td width="75%" valign="top">
                            <p>
                                <a href="https://arxiv.org/abs/2002.05242">
                                    <papertitle>Leveraging Affect Transfer Learning for Behavior Prediction in an Intelligent Tutoring System</papertitle>
                                </a>
                                <br>
                                <strong>Nataniel Ruiz</strong>, Hao Yu, Danielle A. Allessio, Mona Jalal, Thomas Murray, John J. Magee, Jacob R. Whitehill, Vitaly Ablavsky, Ivon Arroyo, Beverly P. Woolf, Stan Sclaroff, Margrit Betke
                                <br>
                                <em>IEEE International Conference on Automatic Face and Gesture Recognition (FG)</em>, 2021
                                <br>
                                <em>IEEE Transactions on Biometrics, Identity and Behavior (T-BIOM)</em>, 2022
                                <br>
                                <a href="https://ieeexplore.ieee.org/abstract/document/9906410">journal paper</a>
                                <br>
                                <font color="red"><strong>(Oral and Best Poster Award - 4% award rate)</strong></font>
                            </p>
                        </td>
                    </tr>
                </table>

                <!-- Disrupting DeepFakes -->
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td width="25%">
                            <img src='figs/img_disrupt.png' width="200" height="200">
                        </td>
                        <td width="75%" valign="top">
                            <p>
                                <a href="https://arxiv.org/abs/2003.01279">
                                    <papertitle>Disrupting DeepFakes: Adversarial Attacks Against Conditional Image Translation Networks and Facial Manipulation Systems</papertitle>
                                </a>
                                <br>
                                <strong>Nataniel Ruiz</strong>, Sarah A. Bargal, Stan Sclaroff
                                <br>
                                <em>CVPR Workshop on Adversarial Machine Learning in Computer Vision and ECCV Workshop on Advances in Image Manipulation</em>, 2020
                                <br>
                                <a href="https://twimlai.com/twiml-talk-375-disrupting-deepfakes-adversarial-attacks-against-conditional-image-translation-networks-with-nataniel-ruiz/">podcast</a> |
                                <a href="https://github.com/natanielruiz/disrupting-deepfakes">code</a> |
                                <a href="https://www.youtube.com/watch?v=7_7r4Ng4-bE&feature=youtu.be">video demo</a>
                              </p>
                            </td>
                            </tr>
                            </table>

                            <!-- Detecting Attended Visual Targets in Video -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tr>
                  <td width="25%">
                      <img src='figs/img_visual_target_video.png' width="200" height="200">
                  </td>
                  <td width="75%" valign="top">
                      <p>
                          <a href="https://arxiv.org/abs/2003.02501">
                              <papertitle>Detecting Attended Visual Targets in Video</papertitle>
                          </a>
                          <br>
                          Eunji Chong, Yongxin Wang, <strong>Nataniel Ruiz</strong>, James M. Rehg
                          <br>
                          <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020
                          <br>
                          <a href="https://github.com/ejcgt/attention-target-detection">code</a>
                      </p>
                  </td>
              </tr>
          </table>

          <!-- Learning To Simulate -->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tr>
                  <td width="25%">
                      <img src='figs/lts_sim.png' width="200" height="200">
                  </td>
                  <td width="75%" valign="top">
                      <p>
                          <a href="https://arxiv.org/abs/1810.02513">
                              <papertitle>Learning To Simulate</papertitle>
                          </a>
                          <br>
                          <strong>Nataniel Ruiz</strong>, Samuel Schulter, Manmohan Chandraker
                          <br>
                          <em>International Conference on Learning Representations (ICLR)</em>, 2019
                          <br>
                          <a href="docs/lts_poster.pdf">poster</a>
                      </p>
                  </td>
              </tr>
          </table>

          <!-- Connecting Gaze, Scene, and Attention -->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tr>
                  <td width="25%">
                      <img src='figs/gaze.png' width="200" height="200">
                  </td>
                  <td width="75%" valign="top">
                      <p>
                          <a href="http://openaccess.thecvf.com/content_ECCV_2018/html/Eunji_Chong_Connecting_Gaze_Scene_ECCV_2018_paper.html">
                              <papertitle>Connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency</papertitle>
                          </a>
                          <br>
                          Eunji Chong, <strong>Nataniel Ruiz</strong>, Yongxin Wang, Yun Zhang, Agata Rozga, James M. Rehg
                          <br>
                          <em>European Conference on Computer Vision (ECCV)</em>, 2018
                          <br>
                          <a href="docs/argos_poster.pdf">poster</a> |
                          <a href="docs/Chong_2018_ECCV.bib">bibtex</a>
                      </p>
                  </td>
              </tr>
          </table>

          <!-- Fine-Grained Head Pose Estimation Without Keypoints -->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tr>
                  <td width="25%">
                      <img src='figs/deep-head-pose.png' width="200" height="200">
                  </td>
                  <td width="75%" valign="top">
                      <p>
                          <a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/w41/html/Ruiz_Fine-Grained_Head_Pose_CVPR_2018_paper.html">
                              <papertitle>Fine-Grained Head Pose Estimation Without Keypoints</papertitle>
                          </a>
                          <br>
                          <strong>Nataniel Ruiz</strong>, Eunji Chong, James M. Rehg
                          <br>
                          <em>Conference on Computer Vision and Pattern Recognition Workshop (CVPRW)</em>, 2018
                          <font color="red"><strong>(Oral)</strong></font>
                          <br>
                          <a href="https://github.com/natanielruiz/deep-head-pose">code</a> |
                          <a href="https://youtu.be/Bz6eF4Nl1O8">video demo</a> |
                          <a href="docs/hopenet_poster.pdf">poster</a> |
                          <a href="docs/ruiz2017fine.bib">bibtex</a>
                      </p>
                  </td>
              </tr>
          </table>

          <!-- Learning to Localize and Align Fine-Grained Actions -->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tr>
                  <td width="25%">
                      <img src='figs/localize-and-align.png' width="200" height="200">
                  </td>
                  <td width="75%" valign="top">
                      <p>
                          <a href="https://arxiv.org/abs/1809.08381">
                              <papertitle>Learning to Localize and Align Fine-Grained Actions to Sparse Instructions</papertitle>
                          </a>
                          <br>
                          Meera Hahn, <strong>Nataniel Ruiz</strong>, Jean-Baptiste Alayrac, Ivan Laptev, James M. Rehg
                          <br>
                          <em>arXiv</em>, 2018
                      </p>
                  </td>
              </tr>
          </table>

          <!-- Detecting Gaze Towards Eyes in Natural Social Interactions -->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tr>
                  <td width="25%">
                      <img src='figs/eye-contact.png' width="200" height="200">
                  </td>
                  <td width="75%" valign="top">
                      <p>
                          <a href="https://dl.acm.org/citation.cfm?id=3131902">
                              <papertitle>Detecting Gaze Towards Eyes in Natural Social Interactions and Its Use in Child Assessment</papertitle>
                          </a>
                          <br>
                          Eunji Chong, Katha Chanda, Zhefan Ye, Audrey Southerland, <strong>Nataniel Ruiz</strong>, Rebecca M. Jones, Agata Rozga, James M. Rehg
                          <br>
                          <em>UbiComp and Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)</em>, 2017
                          <br>
                          <font color="red"><strong>(Oral Presentation and Distinguished Paper Award - 3% award rate)</strong></font>
                          <br>
                          <a href="docs/chong2017imwut.bib">bibtex</a>
                      </p>
                  </td>
              </tr>
          </table>

          <!-- Dockerface -->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tr>
                  <td width="25%">
                      <img src='figs/dockerface.jpg' width="200" height="200">
                  </td>
                  <td width="75%" valign="top">
                      <p>
                          <a href="https://arxiv.org/abs/1708.04370">
                              <papertitle>Dockerface: an Easy to Install and Use Faster R-CNN Face Detector in a Docker Container</papertitle>
                          </a>
                          <br>
                          <strong>Nataniel Ruiz</strong>, James M. Rehg
                          <br>
                          <em>arXiv</em>, 2017
                          <br>
                          <a href="https://github.com/natanielruiz/dockerface">code</a> |
                          <a href="docs/ruiz2017dockerface.bib">bibtex</a>
                      </p>
                  </td>
              </tr>
          </table>

          <!-- Projects -->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tr>
                  <td>
                      <heading>Projects</heading>
                  </td>
              </tr>
          </table>

          <!-- android-yolo -->
          <table width="100%" align="center" border="0" cellpadding="20">
              <tr>
                  <td width="25%">
                      <img src="figs/yolo.png" width="200" height="200">
                  </td>
                  <td width="75%" valign="top">
                      <p>
                          <a href="https://github.com/natanielruiz/android-yolo">
                              <papertitle>android-yolo</papertitle>
                          </a>
                          <br>
                          <strong>Nataniel Ruiz</strong>
                          <br>
                          <a href="https://youtu.be/EhMrf4G5Wf0">video demo</a> |
                          <a href="https://drive.google.com/open?id=0B2fFW2t9-qW3LWFDNXVHUE9rV3M">app apk</a>
                          <br>
                          Real-time object detection on Android using the YOLO network with TensorFlow.
                      </p>
                  </td>
              </tr>
          </table>
      </td>
  </tr>
</table>
</body>
</html>
